{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7d8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a999bf8",
   "metadata": {},
   "source": [
    "**Why Use np.random.seed(0)?**\n",
    "\n",
    "\n",
    "\n",
    "Consistent Results: By fixing the seed, the random number generator produces the same sequence of numbers each time the code is run. This ensures that the results are consistent across different runs, making debugging and verification easier.\n",
    "\n",
    "Fair Comparison: When comparing different models or algorithms, it's essential to start with the same initial conditions. Setting a seed ensures that the randomness (like initial weights, random sampling, etc.) is controlled, allowing a fair comparison.\n",
    "\n",
    "Debugging: Identifying issues and debugging becomes more manageable when the results are reproducible. It allows you to pinpoint where things might be going wrong.\n",
    "\n",
    "Without seed:\n",
    "X1: [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548 ]\n",
    "X2: [0.64589411 0.43758721 0.891773   0.96366276 0.38344152]\n",
    "\n",
    "With seed:\n",
    "X3: [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548 ]\n",
    "X4: [0.5488135  0.71518937 0.60276338 0.54488318 0.4236548 ]\n",
    "\n",
    "In the example, setting the seed ensures that X3 and X4 are identical, demonstrating reproducibility. Without setting the seed, X1 and X2 are different on each run, making results inconsistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9adabbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 random X points\n",
    "X = np.random.rand(1000)\n",
    "\n",
    "# Calculate Y points\n",
    "Y = 2 * (X ** 2) + 3 * X + 5\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(Y, Y_pred):\n",
    "    return np.mean((Y - Y_pred) ** 2)\n",
    "\n",
    "# Initialize parameters\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6a197",
   "metadata": {},
   "source": [
    "# Why Use Epochs\n",
    "\n",
    "**Incremental Learning**\n",
    "\n",
    "In each epoch, the model goes through the entire dataset once. This allows the model to gradually learn the underlying patterns in the data.\n",
    "\n",
    "By repeating the training process multiple times, the model can make incremental updates to the parameters, improving the fit to the data.\n",
    "\n",
    "**Convergence**\n",
    "\n",
    "Gradient descent algorithms typically do not converge to the optimal solution in a single pass through the data.\n",
    "\n",
    "Multiple epochs allow the algorithm to iteratively adjust the parameters, reducing the loss function step by step until it converges to a minimum.\n",
    "\n",
    "**Overcoming Local Minima**\n",
    "\n",
    "The loss surface may have multiple local minima. By training for multiple epochs, the model has a better chance of escaping local minima and finding a more optimal solution.\n",
    "\n",
    "**Improving Accuracy**\n",
    "\n",
    "Early epochs might result in significant reductions in the loss, but the improvements become smaller as training progresses.\n",
    "\n",
    "Sufficient epochs ensure the model parameters are fine-tuned to achieve higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e25faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning rate and epochs\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "patience = 10  # Early stopping patience\n",
    "min_improvement = 1e-5  # Minimum improvement threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23816f59",
   "metadata": {},
   "source": [
    "## The problems you might face with too few or too many epochs\n",
    "\n",
    "**Too few epochs (low value)**\n",
    "\n",
    "\n",
    "Underfitting: The model may not have enough time to learn the patterns in the data, resulting in poor performance on both training and test sets.\n",
    "\n",
    "High bias: The model might be too simplistic and fail to capture the underlying relationships in the data.\n",
    "\n",
    "Incomplete convergence: The optimization process may not reach the optimal point, leading to suboptimal model parameters.\n",
    "\n",
    "Inconsistent results: With very few epochs, the model's performance can be highly variable between different training runs.\n",
    "\n",
    "\n",
    "**Too many epochs (high value)**\n",
    "\n",
    "\n",
    "Overfitting: The model may start to memorize the training data, leading to poor generalization on unseen data.\n",
    "\n",
    "High variance: The model becomes too complex and starts to fit noise in the training data.\n",
    "\n",
    "Increased computational cost: More epochs require more time and resources to train the model.\n",
    "\n",
    "Diminishing returns: After a certain point, additional epochs may not significantly improve performance, wasting computational resources.\n",
    "\n",
    "Potential instability: In some cases, excessive training can lead to oscillations or divergence in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf6ff4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store loss values\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d1a60",
   "metadata": {},
   "source": [
    "**Why Use float('inf')?**\n",
    "\n",
    "Initial Comparison: At the start, best_loss needs to be a value that any real loss value will be lower than. Infinity guarantees this because any finite number is less than infinity.\n",
    "\n",
    "Updating Best Loss: Once the first epoch is completed, the loss value is compared against infinity and will always be smaller, thus updating best_loss to the first calculated loss value. This ensures that best_loss always holds the lowest loss encountered so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe76ce3",
   "metadata": {},
   "source": [
    "**Issues with Negative Infinity**\n",
    "\n",
    "If best_loss is initialized to negative infinity, any calculated loss (which will be a finite positive number or zero) will be greater than negative infinity.\n",
    "\n",
    "The condition if loss < best_loss (which we use to update the best loss) will never be true because no real loss value will be less than negative infinity.\n",
    "\n",
    "Since the condition to update best_loss is never met, best_loss remains negative infinity.\n",
    "\n",
    "The training process would not correctly track the minimum loss, and the logic relying on the best loss for early stopping or other purposes would be faulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28b7f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 30.55923241046619\n",
      "Epoch 2, Loss: 15.89799892249081\n",
      "Epoch 3, Loss: 8.305742952237983\n",
      "Epoch 4, Loss: 4.37237795161445\n",
      "Epoch 5, Loss: 2.3329357352870113\n",
      "Epoch 6, Loss: 1.2739123843340432\n",
      "Epoch 7, Loss: 0.7224967387968035\n",
      "Epoch 8, Loss: 0.43396740026081854\n",
      "Epoch 9, Loss: 0.2816557943966371\n",
      "Epoch 10, Loss: 0.1999943414918051\n",
      "Epoch 11, Loss: 0.1550398828389383\n",
      "Epoch 12, Loss: 0.1292174739474014\n",
      "Epoch 13, Loss: 0.11342421274552869\n",
      "Epoch 14, Loss: 0.10294240136585156\n",
      "Epoch 15, Loss: 0.09532430524454008\n",
      "Epoch 16, Loss: 0.0892973524889253\n",
      "Epoch 17, Loss: 0.08419748320156031\n",
      "Epoch 18, Loss: 0.07967585406788817\n",
      "Epoch 19, Loss: 0.07554702410351533\n",
      "Epoch 20, Loss: 0.07171036745761344\n",
      "Epoch 21, Loss: 0.06810938696892907\n",
      "Epoch 22, Loss: 0.06471064461113142\n",
      "Epoch 23, Loss: 0.061492845620130666\n",
      "Epoch 24, Loss: 0.05844117840360249\n",
      "Epoch 25, Loss: 0.0555443752002293\n",
      "Epoch 26, Loss: 0.05279318140651755\n",
      "Epoch 27, Loss: 0.050179554452633966\n",
      "Epoch 28, Loss: 0.04769624071281471\n",
      "Epoch 29, Loss: 0.04533654849508638\n",
      "Epoch 30, Loss: 0.04309422291485801\n",
      "Epoch 31, Loss: 0.04096337387982637\n",
      "Epoch 32, Loss: 0.038938431924136936\n",
      "Epoch 33, Loss: 0.03701411879900994\n",
      "Epoch 34, Loss: 0.035185426026374665\n",
      "Epoch 35, Loss: 0.033447597883249386\n",
      "Epoch 36, Loss: 0.03179611697330206\n",
      "Epoch 37, Loss: 0.030226691416810045\n",
      "Epoch 38, Loss: 0.028735243143726306\n",
      "Epoch 39, Loss: 0.02731789700995863\n",
      "Epoch 40, Loss: 0.02597097057946303\n",
      "Epoch 41, Loss: 0.024690964478771043\n",
      "Epoch 42, Loss: 0.023474553264300833\n",
      "Epoch 43, Loss: 0.022318576760818484\n",
      "Epoch 44, Loss: 0.02122003183927815\n",
      "Epoch 45, Loss: 0.020176064607880562\n",
      "Epoch 46, Loss: 0.019183962993577212\n",
      "Epoch 47, Loss: 0.018241149693458632\n",
      "Epoch 48, Loss: 0.01734517547704704\n",
      "Epoch 49, Loss: 0.016493712821745457\n",
      "Epoch 50, Loss: 0.01568454986472733\n",
      "Epoch 51, Loss: 0.014915584655459068\n",
      "Epoch 52, Loss: 0.014184819693873113\n",
      "Epoch 53, Loss: 0.01349035673997472\n",
      "Epoch 54, Loss: 0.012830391881383203\n",
      "Epoch 55, Loss: 0.012203210845983652\n",
      "Epoch 56, Loss: 0.011607184547506609\n",
      "Epoch 57, Loss: 0.011040764852458909\n",
      "Epoch 58, Loss: 0.010502480557405953\n",
      "Epoch 59, Loss: 0.009990933566151728\n",
      "Epoch 60, Loss: 0.009504795256883185\n",
      "Epoch 61, Loss: 0.00904280302983926\n",
      "Epoch 62, Loss: 0.008603757026533434\n",
      "Epoch 63, Loss: 0.008186517012005512\n",
      "Epoch 64, Loss: 0.007789999412000689\n",
      "Epoch 65, Loss: 0.0074131744973777655\n",
      "Epoch 66, Loss: 0.007055063708430078\n",
      "Epoch 67, Loss: 0.006714737112166629\n",
      "Epoch 68, Loss: 0.006391310985946455\n",
      "Epoch 69, Loss: 0.00608394552118728\n",
      "Epoch 70, Loss: 0.0057918426411818135\n",
      "Epoch 71, Loss: 0.005514243927351596\n",
      "Epoch 72, Loss: 0.005250428648549507\n",
      "Epoch 73, Loss: 0.004999711888290618\n",
      "Epoch 74, Loss: 0.004761442765044846\n",
      "Epoch 75, Loss: 0.004535002740967176\n",
      "Epoch 76, Loss: 0.004319804014670609\n",
      "Epoch 77, Loss: 0.004115287993865798\n",
      "Epoch 78, Loss: 0.003920923843898327\n",
      "Epoch 79, Loss: 0.003736207108412425\n",
      "Epoch 80, Loss: 0.0035606583985567253\n",
      "Epoch 81, Loss: 0.003393822147326331\n",
      "Epoch 82, Loss: 0.0032352654258042267\n",
      "Epoch 83, Loss: 0.0030845768182261608\n",
      "Epoch 84, Loss: 0.0029413653529460496\n",
      "Epoch 85, Loss: 0.002805259486523922\n",
      "Epoch 86, Loss: 0.0026759061382967356\n",
      "Epoch 87, Loss: 0.0025529697729234704\n",
      "Epoch 88, Loss: 0.0024361315285204473\n",
      "Epoch 89, Loss: 0.0023250883881215643\n",
      "Epoch 90, Loss: 0.00221955239231038\n",
      "Epoch 91, Loss: 0.0021192498909782423\n",
      "Epoch 92, Loss: 0.0020239208322640925\n",
      "Epoch 93, Loss: 0.0019333180868284648\n",
      "Epoch 94, Loss: 0.0018472068057057106\n",
      "Epoch 95, Loss: 0.0017653638100658582\n",
      "Epoch 96, Loss: 0.0016875770113006588\n",
      "Epoch 97, Loss: 0.0016136448599266523\n",
      "Epoch 98, Loss: 0.0015433758218735385\n",
      "Epoch 99, Loss: 0.0014765878807968275\n",
      "Epoch 100, Loss: 0.0014131080651216636\n",
      "Epoch 101, Loss: 0.0013527719985888616\n",
      "Epoch 102, Loss: 0.0012954234731352465\n",
      "Epoch 103, Loss: 0.0012409140429985198\n",
      "Epoch 104, Loss: 0.0011891026389919163\n",
      "Epoch 105, Loss: 0.001139855201946348\n",
      "Epoch 106, Loss: 0.0010930443343676601\n",
      "Epoch 107, Loss: 0.0010485489694036895\n",
      "Epoch 108, Loss: 0.0010062540562611644\n",
      "Epoch 109, Loss: 0.0009660502612548203\n",
      "Epoch 110, Loss: 0.0009278336837120789\n",
      "Epoch 111, Loss: 0.0008915055859950239\n",
      "Epoch 112, Loss: 0.0008569721369381664\n",
      "Epoch 113, Loss: 0.0008241441680353087\n",
      "Epoch 114, Loss: 0.0007929369417420283\n",
      "Epoch 115, Loss: 0.000763269931291739\n",
      "Epoch 116, Loss: 0.000735066611453084\n",
      "Epoch 117, Loss: 0.0007082542596851247\n",
      "Epoch 118, Loss: 0.0006827637671734932\n",
      "Epoch 119, Loss: 0.0006585294592566209\n",
      "Epoch 120, Loss: 0.000635488924775298\n",
      "Epoch 121, Loss: 0.0006135828539023012\n",
      "Epoch 122, Loss: 0.000592754884030541\n",
      "Epoch 123, Loss: 0.0005729514533194242\n",
      "Epoch 124, Loss: 0.0005541216615187546\n",
      "Epoch 125, Loss: 0.0005362171377086293\n",
      "Epoch 126, Loss: 0.0005191919146115981\n",
      "Epoch 127, Loss: 0.0005030023091505341\n",
      "Epoch 128, Loss: 0.0004876068089418582\n",
      "Epoch 129, Loss: 0.00047296596442911344\n",
      "Epoch 130, Loss: 0.0004590422863766893\n",
      "Epoch 131, Loss: 0.00044580014845728856\n",
      "Epoch 132, Loss: 0.0004332056946800388\n",
      "Epoch 133, Loss: 0.0004212267514186893\n",
      "Epoch 134, Loss: 0.0004098327438113274\n",
      "Epoch 135, Loss: 0.00039899461631436\n",
      "Epoch 136, Loss: 0.000388684757204349\n",
      "Epoch 137, Loss: 0.00037887692683150217\n",
      "Epoch 138, Loss: 0.00036954618943841136\n",
      "Epoch 139, Loss: 0.00036066884836685243\n",
      "Epoch 140, Loss: 0.0003522223844843085\n",
      "Epoch 141, Loss: 0.0003441853976702114\n",
      "Epoch 142, Loss: 0.0003365375512098482\n",
      "Epoch 143, Loss: 0.00032925951895145075\n",
      "Epoch 144, Loss: 0.0003223329350891929\n",
      "Epoch 145, Loss: 0.00031574034644153384\n",
      "Epoch 146, Loss: 0.000309465167100984\n",
      "Epoch 147, Loss: 0.00030349163533740874\n",
      "Epoch 148, Loss: 0.0002978047726429073\n",
      "Epoch 149, Loss: 0.00029239034481183567\n",
      "Epoch 150, Loss: 0.0002872348249548332\n",
      "Epoch 151, Loss: 0.0002823253583507868\n",
      "Epoch 152, Loss: 0.0002776497290453606\n",
      "Epoch 153, Loss: 0.0002731963281093095\n",
      "Epoch 154, Loss: 0.0002689541234741269\n",
      "Epoch 155, Loss: 0.0002649126312666067\n",
      "Epoch 156, Loss: 0.00026106188856785096\n",
      "Epoch 157, Loss: 0.00025739242752595957\n",
      "Epoch 158, Loss: 0.0002538952507550879\n",
      "Epoch 159, Loss: 0.00025056180795700935\n",
      "Epoch 160, Loss: 0.0002473839737043683\n",
      "Epoch 161, Loss: 0.0002443540263279669\n",
      "Epoch 162, Loss: 0.00024146462785315626\n",
      "Epoch 163, Loss: 0.00023870880493325042\n",
      "Epoch 164, Loss: 0.00023607993073039598\n",
      "Epoch 165, Loss: 0.0002335717076968147\n",
      "Epoch 166, Loss: 0.0002311781512116849\n",
      "Epoch 167, Loss: 0.0002288935740311499\n",
      "Epoch 168, Loss: 0.00022671257151102552\n",
      "Epoch 169, Loss: 0.00022463000756383462\n",
      "Epoch 170, Loss: 0.00022264100131365723\n",
      "Epoch 171, Loss: 0.0002207409144141384\n",
      "Epoch 172, Loss: 0.0002189253389966792\n",
      "Epoch 173, Loss: 0.00021719008621752438\n",
      "Epoch 174, Loss: 0.00021553117537396088\n",
      "Epoch 175, Loss: 0.00021394482356134408\n",
      "Epoch 176, Loss: 0.00021242743584411547\n",
      "Epoch 177, Loss: 0.00021097559591522056\n",
      "Epoch 178, Loss: 0.0002095860572196876\n",
      "Epoch 179, Loss: 0.00020825573451929938\n",
      "Epoch 180, Loss: 0.0002069816958764285\n",
      "Epoch 181, Loss: 0.00020576115503621412\n",
      "Epoch 182, Loss: 0.00020459146418728698\n",
      "Epoch 183, Loss: 0.00020347010708222823\n",
      "Epoch 184, Loss: 0.00020239469249988263\n",
      "Epoch 185, Loss: 0.00020136294803255826\n",
      "Epoch 186, Loss: 0.0002003727141819319\n",
      "Epoch 187, Loss: 0.00019942193874835894\n",
      "Epoch 188, Loss: 0.00019850867149898472\n",
      "Epoch 189, Loss: 0.0001976310591008061\n",
      "Epoch 190, Loss: 0.0001967873403055231\n",
      "Epoch 191, Loss: 0.0001959758413736687\n",
      "Epoch 192, Loss: 0.0001951949717261121\n",
      "Early stopping at epoch 192\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent loop\n",
    "for epoch in range(epochs):\n",
    "    # Predict Y using the current parameters\n",
    "    Y_pred = a * (X ** 2) + b * X + c\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = loss_function(Y, Y_pred)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Print the current epoch and loss\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss}')\n",
    "    \n",
    "    # Check for early stopping based on minimum improvement\n",
    "    if best_loss - loss > min_improvement:\n",
    "        best_loss = loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch + 1}')\n",
    "        break\n",
    "    \n",
    "    # Calculate gradients\n",
    "    dL_da = -2 * np.mean((Y - Y_pred) * (X ** 2))\n",
    "    dL_db = -2 * np.mean((Y - Y_pred) * X)\n",
    "    dL_dc = -2 * np.mean(Y - Y_pred)\n",
    "    \n",
    "    # Update parameters\n",
    "    a -= learning_rate * dL_da\n",
    "    b -= learning_rate * dL_db\n",
    "    c -= learning_rate * dL_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941693c",
   "metadata": {},
   "source": [
    "**Early Stopping:** The loop monitors the loss and stops training if there is no improvement after a certain number of epochs (patience).\n",
    "\n",
    "**Patience:** If the loss doesn't improve for patience consecutive epochs, training stops.\n",
    "\n",
    "**Loss Plot:** The loss plot shows the loss over the epochs, demonstrating the effect of early stopping.\n",
    "\n",
    "**Minimum Improvement Threshold:** A small threshold (min_improvement) is set to ensure that the loss must improve by at least this amount to reset the patience counter.\n",
    "\n",
    "**Detailed Monitoring:** Prints the loss at each epoch and will indicate if early stopping occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af584e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final values - a: 2.1725580184897906, b: 2.8079775242750684, c: 5.038953560209342\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the final values of a, b, and c\n",
    "print(f'Final values - a: {a}, b: {b}, c: {c}')\n",
    "\n",
    "# Plot the loss over epochs\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba0812b",
   "metadata": {},
   "source": [
    "**Convergence**\n",
    "\n",
    "Convergence in the context of machine learning and optimization algorithms refers to the point where the model's parameters have stabilized and the loss function (or error) stops decreasing significantly with further iterations. In simpler terms, it means the model has learned the underlying patterns in the data as well as it can, and further training won't yield substantial improvements.\n",
    "\n",
    "**Why Convergence Matters:**\n",
    "\n",
    "Efficient Training: It indicates that the model has learned the data sufficiently, so we can stop training to save computational resources.\n",
    "Avoid Overfitting: Continued training beyond convergence can lead to overfitting, where the model starts memorizing the training data instead of generalizing to new, unseen data.\n",
    "Model Performance: Convergence helps in achieving the best possible performance on the training data, which is crucial for the model's predictive capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52550284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
